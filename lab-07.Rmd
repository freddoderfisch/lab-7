---
title: "Lab 07 - Modelling course evaluations"
author: "Frederick Elson"
date: "`r Sys.Date()`"
output: html_document
---

### Packages and Data

```{r load-packages, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)

```


```{r read-data}
evals<-read.csv("data/evals.csv", row.names=1)
```


# Exercise 1: Exploratory Data Analysis

1.  Visualize the distribution of `score` in the dataframe `evals`.

```{r viz-score}
evals %>%
  ggplot(mapping = aes(x = score)) + geom_boxplot() + labs(
    x = "Score",
    title = "distribution of score")
```
The distribution is skewed towards the top end of the scale (5) as people will decide what course to take and therefore will generally take a course they enjoy and give it a high score.

2.  Visualize and describe the relationship between `score` and `bty_avg` using `geom_point()` to represent the data. 

```{r scatterplot}
evals %>%
  ggplot(mapping = aes(x = score, y = bty_avg)) + geom_point()

evals %>%
  ggplot(mapping = aes(x = score, y = bty_avg)) + geom_jitter() + geom_smooth(method = lm)
```

jitter can be used on discrete variables to move the points slighty creating a small amount of random variation to the points and helping make the data easier to visualise and avoid plotting multiple points in the same place.

There is arguably a slight positive correlation, but I don't think this is enough to validate that beauty affects the score.

# Exercise 2: Simple Linear regression with a numerical predictor

1. Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` from average beauty rating (`bty_avg`). Print the regression output using `tidy()`.

```{r fit-score_bty_fit}
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals)
```

```{r tidy-score_bty_fit}
tidy(score_bty_fit)
```

score = 0.0666bty_avg + 3.88

2. Plot the data again using `geom_jitter()`, and add the regression line.

```{r viz-score_bty_fit}
evals %>%
  ggplot(mapping = aes(y = score, x = bty_avg)) + geom_jitter() + geom_smooth(method = lm)
```

3. Interpret the slope of the linear model in context of the data.

The slope is positive but close to 0. This suggests that there is very little correlation between beauty and the rating of the professor. The standard error is also quite large therefore this model is not very accurate more data could be beneficial for a more accurate model.

4. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.

The score is 3.88 when the beauty average given to a professor is 0. As the beauty average is a scale between 1 and 10 this makes little sence in this context.

5. Determine the $R^2$ of the model and interpret it in the context of the data.

```{r R2}
glance(score_bty_fit)$r.squared
```

very low value of R^2 therefore the linear model is very bad at predicting the score based on the beauty average.

6. Make a plot of residuals vs. predicted values for the model above.

```{r viz-score_bty_fit-diagnostic}
score_bty_aug <- augment(score_bty_fit$fit)
score_bty_aug %>%
ggplot(mapping = aes(y = .resid, x = .fitted)) + geom_jitter() + geom_hline(yintercept = 0, linetype = "dashed") + labs(y = "residuals",                                                                            x = "predicted values")
```
They are not completely randomly distributed around 0 they are stacked above the y intercept. 

# Exercise 3: Simple Linear regression with a categorical predictor

0. Look at the variable rank, and determine the frequency of each category level.

```{r}

```

1. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.

```{r fit-score_rank_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

2. Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor. 

```{r fit-score_gender_fit}
# fit model

# tidy model output
```

```{r score_gender_intercept, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_intercept <- tidy(score_gender_fit) %>% 
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  pull()
```

```{r score_gender_slope, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_slope <- tidy(score_gender_fit) %>% 
  filter(term == "gendermale") %>%
  select(estimate) %>%
  pull()
```

*Add your narrative here. Use in-line code!*

# Exercise 4: Multiple linear regression

1. Fit a multiple linear regression model, predicting average professor evaluation `score` based on average beauty rating (`bty_avg`) and `gender.`

```{r fit-score_bty_gender_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

```{r eval = FALSE}
ggplot(___) + ...
```

2. What percent of the variability in `score` is explained by the model `score_bty_gender_fit`. 

```{r}
# ...
```


3. What is the equation of the line corresponding to just male professors?

*Add your equation here.*

4. For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

*Add your narrative here.*

5. How does the relationship between beauty and evaluation score vary between male and female professors?

*Add your narrative here.*

6. How do the adjusted $R^2$ values of `score_bty_fit` and `score_bty_gender_fit` compare? 

```{r eval=FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(___)$adj.r.squared
glance(___)$adj.r.squared
```

*Add your narrative here.*

7. Compare the slopes of `bty_avg` under the two models (`score_bty_fit` and `score_bty_gender_fit`).

*Add your narrative here.*

# Exercise 5: Interpretation of log-transformed response variables

If you do not know how to use LaTeX, do this exercise with pen and paper.
